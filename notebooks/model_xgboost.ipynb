{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51c32021",
   "metadata": {},
   "source": [
    "XGBOOST IS KNOWN FOR SPEED AND PERFORMANCE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4edbbfbe",
   "metadata": {},
   "source": [
    "### **Next Steps**\n",
    "- Experiment with different models such as XGBoost\n",
    "- Tune hyperparameters to improve performance.\n",
    "- Try feature engineering to enhance the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fd0ef74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\creed\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import KFold\n",
    "import time\n",
    "import optuna\n",
    "import joblib # for saving the model\n",
    "\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d797bc",
   "metadata": {},
   "source": [
    "LETS EXPLORE THE DATA TO ADVANCE LEVEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78db35cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (6195, 31)\n",
      "y_train shape: (6195, 11)\n"
     ]
    }
   ],
   "source": [
    "X = pd.read_csv('X_processed.csv')\n",
    "X_test = pd.read_csv('X_test_processed.csv')\n",
    "y = pd.read_csv('y_processed.csv')\n",
    "\n",
    "# Drop if still in the data\n",
    "if 'PID' in X.columns:\n",
    "    X = X.drop(columns=['PID'])\n",
    "if 'site' in X.columns:\n",
    "    X = X.drop(columns=['site'])\n",
    "\n",
    "if 'PID' in X_test.columns:\n",
    "    X_test = X_test.drop(columns=['PID'])\n",
    "if 'site' in X_test.columns:\n",
    "    X_test = X_test.drop(columns=['site'])\n",
    "\n",
    "\n",
    "#split the data into training and validation sets \n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c4316d",
   "metadata": {},
   "source": [
    "LETS CREATE A FRAMEWORK TO EVLAUTE A MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f74f38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to  evalute  model constently\n",
    "\n",
    "def evaluate_model(model,X_train,y_train,X_val,y_val,model_name,):\n",
    "      #tracking training time \n",
    "    StartTime = time.time()\n",
    "    \n",
    "    #fit the model\n",
    "    model.fit(X_train,y_train)\n",
    "\n",
    "    #trainin the time\n",
    "    trainTime = time.time()-StartTime\n",
    "\n",
    "    #prediction\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_val = model.predict(X_val)\n",
    "\n",
    "\n",
    "    #check the errors\n",
    "    train_mae = mean_absolute_error(y_train,y_pred_train)\n",
    "    val_mae = mean_absolute_error(y_val,y_pred_val)\n",
    "\n",
    "\n",
    "    #check the RMSE\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train,y_pred_train))\n",
    "    val_rmse = np.sqrt(mean_squared_error(y_val,y_pred_val))\n",
    "   \n",
    "\n",
    "     # Print results\n",
    "    print(f\"\\n{model_name} Results:\")\n",
    "    print(f\"Training Time: {trainTime:.2f} seconds\")\n",
    "    print(f\"Training MAE: {train_mae:.4f}, RMSE: {train_rmse:.4f}\")\n",
    "    print(f\"Validation MAE: {val_mae:.4f}, RMSE: {val_rmse:.4f}\")\n",
    "\n",
    "    # Return the results\n",
    "    return {\n",
    "        'model': model,\n",
    "        'name': model_name,\n",
    "        'train_mae': train_mae,\n",
    "        'val_mae': val_mae,\n",
    "        'train_rmse': train_rmse,\n",
    "        'val_rmse': val_rmse,\n",
    "        'train_time': trainTime\n",
    "    }\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707095b3",
   "metadata": {},
   "source": [
    "XGBOOST MODEL WITH HYPERPARAMETER TUNNING\n",
    "THEY ARE TOOLS USED IN SETTING THE BEHAVIOR OF TH MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35ab31af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#xgboost model using the framework optuna\n",
    "#Define obiective function for optuna\n",
    "\n",
    "def get_objective_xgb(X_train, y_train, X_val, y_val):\n",
    "  def objective_xgb(trial):\n",
    "    #define the hyperparameters\n",
    "    params = {\n",
    "        'n_estimators':trial.suggest_int('n_estimators',50,500)#how many tress to use\n",
    "        ,\"max_depth\":trial.suggest_int('max_depth',3,10)#how deep the tree can go\n",
    "        ,'learning_rate':trial.suggest_float('learning_rate',0.01,0.3)#how fast the model learns\n",
    "        ,'subsample': trial.suggest_float('subsample', 0.6, 1.0)#controls randomness and reduce overfiting \n",
    "        ,'colsample_bytree':trial.suggest_float('colsample_bytree',0.6,1.0)#controls the number of features tp use for each tree\n",
    "        ,'min_child_weight': trial.suggest_int('min_child_weight', 1, 10)#minimum sum of instance weight\n",
    "        ,'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 10.0)#L1 regularization term\n",
    "        ,'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 10.0)#l2 regularization term \n",
    "        ,'random_state':42 #for reproductibility\n",
    "    }\n",
    "\n",
    "    #create the model of multioutput regressor\n",
    "    #since xbgregressor is a single target we wap it on with multioutput rregressor to predict multiple targets\n",
    "   \n",
    "    xgb_model = MultiOutputRegressor(xgb.XGBRegressor(**params))\n",
    "    \n",
    "    #train and fit  the model \n",
    "    xgb_model.fit(X_train,y_train)\n",
    "\n",
    "  \n",
    "\n",
    "     #make predictions\n",
    "    y_pred = xgb_model.predict(X_val)\n",
    "\n",
    "    #check the errors\n",
    "    Mae = mean_absolute_error(y_val,y_pred)\n",
    "\n",
    "    return Mae\n",
    "  return objective_xgb\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b004c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: c:\\Users\\creed\\OneDrive\\Documents\\GitHub\\Amini-Soil-prediction\\notebooks\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "print('saved to:',os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd5dcd21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-28 01:02:38,771] A new study created in memory with name: no-name-2033c442-401f-4c60-88f7-3af4721a6c69\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning XGBoost hyperparameters...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-28 01:02:57,329] Trial 0 finished with value: 160.5139923095703 and parameters: {'n_estimators': 154, 'max_depth': 6, 'learning_rate': 0.1498710211879789, 'subsample': 0.8914060114387905, 'colsample_bytree': 0.8938603790324549, 'min_child_weight': 1, 'reg_alpha': 8.385903128181834, 'reg_lambda': 1.9211224628380341}. Best is trial 0 with value: 160.5139923095703.\n",
      "[I 2025-07-28 01:05:34,309] Trial 1 finished with value: 157.76116943359375 and parameters: {'n_estimators': 317, 'max_depth': 10, 'learning_rate': 0.06209089413840654, 'subsample': 0.9731408637767737, 'colsample_bytree': 0.8001844858002015, 'min_child_weight': 2, 'reg_alpha': 0.9310955068919124, 'reg_lambda': 1.390805275432394}. Best is trial 1 with value: 157.76116943359375.\n",
      "[I 2025-07-28 01:06:40,702] Trial 2 finished with value: 166.58709716796875 and parameters: {'n_estimators': 275, 'max_depth': 9, 'learning_rate': 0.27322732678213646, 'subsample': 0.9049023414050906, 'colsample_bytree': 0.7069752601632435, 'min_child_weight': 2, 'reg_alpha': 1.6292499037162167, 'reg_lambda': 3.8600424915390597}. Best is trial 1 with value: 157.76116943359375.\n",
      "[I 2025-07-28 01:06:49,804] Trial 3 finished with value: 160.4458465576172 and parameters: {'n_estimators': 208, 'max_depth': 5, 'learning_rate': 0.06375960895588642, 'subsample': 0.8891528219994547, 'colsample_bytree': 0.6058964873237421, 'min_child_weight': 4, 'reg_alpha': 2.381413229405239, 'reg_lambda': 4.797365670048355}. Best is trial 1 with value: 157.76116943359375.\n",
      "[I 2025-07-28 01:08:13,194] Trial 4 finished with value: 169.2503204345703 and parameters: {'n_estimators': 333, 'max_depth': 10, 'learning_rate': 0.23177398923703316, 'subsample': 0.7754003781742366, 'colsample_bytree': 0.9756826895010723, 'min_child_weight': 5, 'reg_alpha': 4.853756667655902, 'reg_lambda': 1.9744581345103374}. Best is trial 1 with value: 157.76116943359375.\n",
      "[I 2025-07-28 01:08:33,578] Trial 5 finished with value: 169.02789306640625 and parameters: {'n_estimators': 163, 'max_depth': 7, 'learning_rate': 0.2871064398382111, 'subsample': 0.7172095043250295, 'colsample_bytree': 0.6525855564554355, 'min_child_weight': 7, 'reg_alpha': 3.1678932732771248, 'reg_lambda': 9.896152941047516}. Best is trial 1 with value: 157.76116943359375.\n",
      "[I 2025-07-28 01:08:39,756] Trial 6 finished with value: 163.7136688232422 and parameters: {'n_estimators': 83, 'max_depth': 5, 'learning_rate': 0.06643184693139535, 'subsample': 0.7725138725531968, 'colsample_bytree': 0.7901054906255436, 'min_child_weight': 3, 'reg_alpha': 5.339256643898541, 'reg_lambda': 4.513214582158795}. Best is trial 1 with value: 157.76116943359375.\n",
      "[I 2025-07-28 01:09:55,321] Trial 7 finished with value: 156.6277313232422 and parameters: {'n_estimators': 421, 'max_depth': 7, 'learning_rate': 0.019363899570497556, 'subsample': 0.7622718977762688, 'colsample_bytree': 0.9635535700444304, 'min_child_weight': 1, 'reg_alpha': 1.7301739338630806, 'reg_lambda': 6.514324152089582}. Best is trial 7 with value: 156.6277313232422.\n",
      "[I 2025-07-28 01:10:23,833] Trial 8 finished with value: 166.26980590820312 and parameters: {'n_estimators': 434, 'max_depth': 5, 'learning_rate': 0.1932901355556334, 'subsample': 0.7290571232929068, 'colsample_bytree': 0.6897885711633446, 'min_child_weight': 10, 'reg_alpha': 0.27849749443474403, 'reg_lambda': 7.922500009929073}. Best is trial 7 with value: 156.6277313232422.\n",
      "[I 2025-07-28 01:11:14,047] Trial 9 finished with value: 172.87451171875 and parameters: {'n_estimators': 431, 'max_depth': 6, 'learning_rate': 0.2800850743310628, 'subsample': 0.7438007461492794, 'colsample_bytree': 0.830658957056305, 'min_child_weight': 8, 'reg_alpha': 6.113596033298034, 'reg_lambda': 2.3425980372528974}. Best is trial 7 with value: 156.6277313232422.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best XGBoost Parameters: {'n_estimators': 421, 'max_depth': 7, 'learning_rate': 0.019363899570497556, 'subsample': 0.7622718977762688, 'colsample_bytree': 0.9635535700444304, 'min_child_weight': 1, 'reg_alpha': 1.7301739338630806, 'reg_lambda': 6.514324152089582}\n",
      "Best XGBoost MAE: 156.6277313232422\n",
      "\n",
      "XGBoost Results:\n",
      "Training Time: 82.08 seconds\n",
      "Training MAE: 92.5507, RMSE: 269.8127\n",
      "Validation MAE: 156.6277, RMSE: 474.1683\n"
     ]
    }
   ],
   "source": [
    "# Run the hyperparameter optimization\n",
    "print(\"Tuning XGBoost hyperparameters...\")\n",
    "study_xgb = optuna.create_study(direction='minimize')\n",
    "objective = get_objective_xgb(X_train, y_train, X_val, y_val)\n",
    "study_xgb.optimize(objective, n_trials=10)  # Adjust n_trials as needed\n",
    "\n",
    "print(\"Best XGBoost Parameters:\", study_xgb.best_params)\n",
    "print(\"Best XGBoost MAE:\", study_xgb.best_value)\n",
    "\n",
    "# Create the optimized XGBoost model\n",
    "best_xgb_model = MultiOutputRegressor(xgb.XGBRegressor(**study_xgb.best_params, random_state=42))\n",
    "\n",
    "# Evaluate XGBoost model\n",
    "xgb_results = evaluate_model(best_xgb_model, X_train, y_train, X_val, y_val, \"XGBoost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ab0021b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"xgb_results.pkl\", \"wb\") as f:\n",
    "    pickle.dump(xgb_results, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
