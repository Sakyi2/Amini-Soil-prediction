{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51c32021",
   "metadata": {},
   "source": [
    "XGBOOST IS KNOWN FOR SPEED AND PERFORMANCE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4edbbfbe",
   "metadata": {},
   "source": [
    "### **Next Steps**\n",
    "- Experiment with different models such as XGBoost\n",
    "- Tune hyperparameters to improve performance.\n",
    "- Try feature engineering to enhance the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fd0ef74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\creed\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import KFold\n",
    "import time\n",
    "import optuna\n",
    "\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d797bc",
   "metadata": {},
   "source": [
    "LETS EXPLORE THE DATA TO ADVANCE LEVEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78db35cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (6195, 31)\n",
      "y_train shape: (6195, 11)\n"
     ]
    }
   ],
   "source": [
    "X = pd.read_csv('X_processed.csv')\n",
    "X_test = pd.read_csv('X_test_processed.csv')\n",
    "y = pd.read_csv('y_processed.csv')\n",
    "\n",
    "# Drop if still in the data\n",
    "if 'PID' in X.columns:\n",
    "    X = X.drop(columns=['PID'])\n",
    "if 'site' in X.columns:\n",
    "    X = X.drop(columns=['site'])\n",
    "\n",
    "if 'PID' in X_test.columns:\n",
    "    X_test = X_test.drop(columns=['PID'])\n",
    "if 'site' in X_test.columns:\n",
    "    X_test = X_test.drop(columns=['site'])\n",
    "\n",
    "\n",
    "#split the data into training and validation sets \n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c4316d",
   "metadata": {},
   "source": [
    "LETS CREATE A FRAMEWORK TO EVLAUTE A MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f74f38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to  evalute  model constently\n",
    "\n",
    "def evaluate_model(model,X_train,y_train,X_val,y_val,model_name,):\n",
    "      #tracking training time \n",
    "    StartTime = time.time()\n",
    "    \n",
    "    #fit the model\n",
    "    model.fit(X_train,y_train)\n",
    "\n",
    "    #trainin the time\n",
    "    trainTime = time.time()-StartTime\n",
    "\n",
    "    #prediction\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_val = model.predict(X_val)\n",
    "\n",
    "\n",
    "    #check the errors\n",
    "    train_mae = mean_absolute_error(y_train,y_pred_train)\n",
    "    val_mae = mean_absolute_error(y_val,y_pred_val)\n",
    "\n",
    "\n",
    "    #check the RMSE\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train,y_pred_train))\n",
    "    val_rmse = np.sqrt(mean_squared_error(y_val,y_pred_val))\n",
    "   \n",
    "\n",
    "     # Print results\n",
    "    print(f\"\\n{model_name} Results:\")\n",
    "    print(f\"Training Time: {trainTime:.2f} seconds\")\n",
    "    print(f\"Training MAE: {train_mae:.4f}, RMSE: {train_rmse:.4f}\")\n",
    "    print(f\"Validation MAE: {val_mae:.4f}, RMSE: {val_rmse:.4f}\")\n",
    "\n",
    "    # Return the results\n",
    "    return {\n",
    "        'model': model,\n",
    "        'name': model_name,\n",
    "        'train_mae': train_mae,\n",
    "        'val_mae': val_mae,\n",
    "        'train_rmse': train_rmse,\n",
    "        'val_rmse': val_rmse,\n",
    "        'train_time': trainTime\n",
    "    }\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707095b3",
   "metadata": {},
   "source": [
    "XGBOOST MODEL WITH HYPERPARAMETER TUNNING\n",
    "THEY ARE TOOLS USED IN SETTING THE BEHAVIOR OF TH MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35ab31af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-07 02:05:40,030] A new study created in memory with name: no-name-48dbdf05-00dd-4720-a7f9-ca757e509db1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tunning xgboost hyperparameter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-07 02:05:51,516] Trial 0 finished with value: 163.81642150878906 and parameters: {'n_estimators': 392, 'max_depth': 4, 'learning_rate': 0.1740155897487656, 'subsample': 0.8464387587994547, 'colsample_bytree': 0.9802973048705034, 'min_child_weight': 4, 'reg_alpha': 3.2583388217501295, 'reg_lambda': 9.6778674183621}. Best is trial 0 with value: 163.81642150878906.\n",
      "[I 2025-07-07 02:06:17,250] Trial 1 finished with value: 159.65391540527344 and parameters: {'n_estimators': 342, 'max_depth': 6, 'learning_rate': 0.11614053644429538, 'subsample': 0.828164696839107, 'colsample_bytree': 0.9898418028542564, 'min_child_weight': 1, 'reg_alpha': 0.14943952574635877, 'reg_lambda': 7.120749296186429}. Best is trial 1 with value: 159.65391540527344.\n",
      "[I 2025-07-07 02:06:33,689] Trial 2 finished with value: 171.03424072265625 and parameters: {'n_estimators': 471, 'max_depth': 4, 'learning_rate': 0.2834714986785285, 'subsample': 0.8110985601169758, 'colsample_bytree': 0.8947036805588438, 'min_child_weight': 2, 'reg_alpha': 8.908562804784983, 'reg_lambda': 7.676975174810789}. Best is trial 1 with value: 159.65391540527344.\n",
      "[I 2025-07-07 02:06:58,620] Trial 3 finished with value: 169.67213439941406 and parameters: {'n_estimators': 375, 'max_depth': 6, 'learning_rate': 0.23961455162819315, 'subsample': 0.7412322464857691, 'colsample_bytree': 0.7127314617036506, 'min_child_weight': 9, 'reg_alpha': 1.083525916000232, 'reg_lambda': 9.890077110814342}. Best is trial 1 with value: 159.65391540527344.\n",
      "[I 2025-07-07 02:07:53,186] Trial 4 finished with value: 175.9064178466797 and parameters: {'n_estimators': 434, 'max_depth': 9, 'learning_rate': 0.27814381797177584, 'subsample': 0.7475760059013684, 'colsample_bytree': 0.6872587470994506, 'min_child_weight': 8, 'reg_alpha': 5.305625478171011, 'reg_lambda': 0.23330251080526665}. Best is trial 1 with value: 159.65391540527344.\n",
      "[I 2025-07-07 02:08:37,125] Trial 5 finished with value: 161.21299743652344 and parameters: {'n_estimators': 214, 'max_depth': 10, 'learning_rate': 0.15710645030460005, 'subsample': 0.964985480720998, 'colsample_bytree': 0.7130119318537149, 'min_child_weight': 6, 'reg_alpha': 9.730850497662386, 'reg_lambda': 2.2907859736515412}. Best is trial 1 with value: 159.65391540527344.\n",
      "[I 2025-07-07 02:08:49,479] Trial 6 finished with value: 165.80870056152344 and parameters: {'n_estimators': 441, 'max_depth': 3, 'learning_rate': 0.051752132000970806, 'subsample': 0.8197099271454095, 'colsample_bytree': 0.9221162177309967, 'min_child_weight': 6, 'reg_alpha': 5.49207774726302, 'reg_lambda': 5.262032823761386}. Best is trial 1 with value: 159.65391540527344.\n",
      "[I 2025-07-07 02:08:52,587] Trial 7 finished with value: 174.5810546875 and parameters: {'n_estimators': 108, 'max_depth': 3, 'learning_rate': 0.08382132297816296, 'subsample': 0.7743512494180503, 'colsample_bytree': 0.6266834122797007, 'min_child_weight': 2, 'reg_alpha': 8.091036267688816, 'reg_lambda': 9.072498239193806}. Best is trial 1 with value: 159.65391540527344.\n",
      "[I 2025-07-07 02:09:40,534] Trial 8 finished with value: 157.08587646484375 and parameters: {'n_estimators': 489, 'max_depth': 7, 'learning_rate': 0.07634824088301091, 'subsample': 0.7081509898915024, 'colsample_bytree': 0.9042825182383529, 'min_child_weight': 8, 'reg_alpha': 1.4844375869510362, 'reg_lambda': 9.717618159757942}. Best is trial 8 with value: 157.08587646484375.\n",
      "[I 2025-07-07 02:09:53,218] Trial 9 finished with value: 161.71405029296875 and parameters: {'n_estimators': 222, 'max_depth': 6, 'learning_rate': 0.1501507054392635, 'subsample': 0.6858832777066093, 'colsample_bytree': 0.8021200005426085, 'min_child_weight': 10, 'reg_alpha': 3.053713798825326, 'reg_lambda': 5.062258302218679}. Best is trial 8 with value: 157.08587646484375.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best XGBoost Parameters: {'n_estimators': 489, 'max_depth': 7, 'learning_rate': 0.07634824088301091, 'subsample': 0.7081509898915024, 'colsample_bytree': 0.9042825182383529, 'min_child_weight': 8, 'reg_alpha': 1.4844375869510362, 'reg_lambda': 9.717618159757942}\n",
      "Best XGBoost MAE: 157.08587646484375\n",
      "\n",
      "XGBoost Results:\n",
      "Training Time: 45.51 seconds\n",
      "Training MAE: 38.5811, RMSE: 115.9903\n",
      "Validation MAE: 157.0859, RMSE: 476.6529\n"
     ]
    }
   ],
   "source": [
    "#xgboost model using the framework optuna\n",
    "#Define obiective function for optuna\n",
    "\n",
    "def get_objective_xgb(X_train, y_train, X_val, y_val):\n",
    "  def objective_xgb(trial):\n",
    "    #define the hyperparameters\n",
    "    params = {\n",
    "        'n_estimators':trial.suggest_int('n_estimators',50,500)#how many tress to use\n",
    "        ,\"max_depth\":trial.suggest_int('max_depth',3,10)#how deep the tree can go\n",
    "        ,'learning_rate':trial.suggest_float('learning_rate',0.01,0.3)#how fast the model learns\n",
    "        ,'subsample': trial.suggest_float('subsample', 0.6, 1.0)#controls randomness and reduce overfiting \n",
    "        ,'colsample_bytree':trial.suggest_float('colsample_bytree',0.6,1.0)#controls the number of features tp use for each tree\n",
    "        ,'min_child_weight': trial.suggest_int('min_child_weight', 1, 10)#minimum sum of instance weight\n",
    "        ,'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 10.0)#L1 regularization term\n",
    "        ,'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 10.0)#l2 regularization term \n",
    "        ,'random_state':42 #for reproductibility\n",
    "    }\n",
    "\n",
    "    #create the model of multioutput regressor\n",
    "    #since xbgregressor is a single target we wap it on with multioutput rregressor to predict multiple targets\n",
    "   \n",
    "    xgb_model = MultiOutputRegressor(xgb.XGBRegressor(**params))\n",
    "    \n",
    "    #train and fit  the model \n",
    "    xgb_model.fit(X_train,y_train)\n",
    "\n",
    "     #make predictions\n",
    "    y_pred = xgb_model.predict(X_val)\n",
    "\n",
    "    #check the errors\n",
    "    Mae = mean_absolute_error(y_val,y_pred)\n",
    "\n",
    "    return Mae\n",
    "  return objective_xgb\n",
    "\n",
    "#runinn the optuna process\n",
    "  \n",
    "print('tunning xgboost hyperparameter')\n",
    "study_xgb = optuna.create_study(direction='minimize')\n",
    "objective = get_objective_xgb(X_train, y_train, X_val, y_val)\n",
    "study_xgb.optimize(objective, n_trials=10) #ajust trail are needed\n",
    "\n",
    "\n",
    "print(\"Best XGBoost Parameters:\", study_xgb.best_params)\n",
    "print(\"Best XGBoost MAE:\", study_xgb.best_value)\n",
    "\n",
    "#create a optimaxize xgboost model\n",
    "best_xgb_model = MultiOutputRegressor(xgb.XGBRegressor(**study_xgb.best_params, random_state=42))\n",
    "\n",
    "#evaluate the result\n",
    "xgb_results = evaluate_model(best_xgb_model,X_train, y_train, X_val, y_val, \"XGBoost\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
